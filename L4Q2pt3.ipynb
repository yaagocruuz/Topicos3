{
  "cells": [
    {
      "metadata": {
        "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
        "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
        "trusted": true
      },
      "cell_type": "code",
      "source": "# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"../input\"))\n\n# Any results you write to the current directory are saved as output.",
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": "['glove-global-vectors-for-word-representation', 'imdb-raw']\n",
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "27f7e631d926f85ef910361f63a8f6979fd7b19c"
      },
      "cell_type": "code",
      "source": "from keras.preprocessing.text import Tokenizer\nfrom keras.preprocessing.sequence import pad_sequences\nimport numpy as np\nfrom keras.models import Sequential\nfrom keras.layers import Embedding, Flatten, Dense, Bidirectional\nimport matplotlib.pyplot as plt\nfrom keras.layers import LSTM",
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": "Using TensorFlow backend.\n",
          "name": "stderr"
        }
      ]
    },
    {
      "metadata": {
        "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
        "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a",
        "trusted": true
      },
      "cell_type": "code",
      "source": "imdb_dir = '../input/imdb-raw/aclimdb/aclImdb'\ntrain_dir = os.path.join(imdb_dir, 'test')\nlabels = []\ntexts = []\nfor label_type in ['neg', 'pos']:\n    dir_name = os.path.join(train_dir, label_type)\n    for fname in os.listdir(dir_name):\n        if fname[-4:] == '.txt':\n            f = open(os.path.join(dir_name, fname))\n            texts.append(f.read())\n            f.close()\n            if label_type == 'neg':\n                labels.append(0)\n            else:\n                labels.append(1)",
      "execution_count": 3,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "8792e114d209429e1e057cc47b56b78bcde82d99",
        "trusted": true
      },
      "cell_type": "code",
      "source": "maxlen = 300 # Cuts off reviews after 100 words\ntraining_samples = 5000 # Trains on 200 samples\nvalidation_samples = 10000 # Validates on 10,000 samples\nmax_words = 10000 # Considers only the top 10,000 words in the dataset",
      "execution_count": 4,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "a0b0879b33bf03c875f7da4f9a64b4ad09960297",
        "trusted": true
      },
      "cell_type": "code",
      "source": "tokenizer = Tokenizer(num_words=max_words)\ntokenizer.fit_on_texts(texts)\nsequences = tokenizer.texts_to_sequences(texts)",
      "execution_count": 5,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "cb48397ef0cc60ff54a93a8e6223e264d4d132aa",
        "trusted": true
      },
      "cell_type": "code",
      "source": "word_index = tokenizer.word_index\nprint('Found %s unique tokens.' % len(word_index))",
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": "Found 72633 unique tokens.\n",
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "_uuid": "136556a707dc071cf4b7c59fb86ed2686b02d1d9",
        "trusted": true
      },
      "cell_type": "code",
      "source": "data = pad_sequences(sequences, maxlen=maxlen)",
      "execution_count": 7,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "813bf1f79191e5bdcde5559d912b41cd4a6e7f09",
        "trusted": true
      },
      "cell_type": "code",
      "source": "labels = np.asarray(labels)\nprint('Shape of data tensor:', data.shape)\nprint('Shape of label tensor:', labels.shape)",
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": "Shape of data tensor: (17243, 300)\nShape of label tensor: (17243,)\n",
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "_uuid": "2f3b943de7645b66fa875b742ecc4799c59a1599"
      },
      "cell_type": "markdown",
      "source": "### Splits the data into a training set and a validation set, but first shuffles the data, because you’re starting with data in which samples are ordered (all negative first, then all positive) "
    },
    {
      "metadata": {
        "_uuid": "e6f2e1096f2c6703a47b118da4906ee2f8a149b9",
        "trusted": true
      },
      "cell_type": "code",
      "source": "indices = np.arange(data.shape[0])\nnp.random.shuffle(indices)\ndata = data[indices]\nlabels = labels[indices]",
      "execution_count": 9,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "f67a2e2317c66a3dcd9f557e6a5c0f99c5625ad3",
        "trusted": true
      },
      "cell_type": "code",
      "source": "x_train = data[:training_samples]\ny_train = labels[:training_samples]\nx_val = data[training_samples: training_samples + validation_samples]\ny_val = labels[training_samples: training_samples + validation_samples]",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "7e0542e228e223e1d450a98996a5c62eb9a1cdd6"
      },
      "cell_type": "markdown",
      "source": "### Let’s parse the unzipped file (a .txt file) to build an index that maps words (as strings) to their vector representation (as number vectors)."
    },
    {
      "metadata": {
        "_uuid": "a4d7b19407b854c7f79774ba940c44d8f9204f9e",
        "trusted": true
      },
      "cell_type": "code",
      "source": "glove_dir = '../input/glove-global-vectors-for-word-representation/'\nembeddings_index = {}\nf = open(os.path.join(glove_dir, 'glove.6B.100d.txt'))\nfor line in f:\n    values = line.split()\n    word = values[0]\n    coefs = np.asarray(values[1:], dtype='float32')\n    embeddings_index[word] = coefs\nf.close()\nprint('Found %s word vectors.' % len(embeddings_index))",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "ea509caa576c749c9465e65c4d01ec973fd1db6a",
        "trusted": true
      },
      "cell_type": "code",
      "source": "embedding_dim = 100\nembedding_matrix = np.zeros((max_words, embedding_dim))\nfor word, i in word_index.items():\n    if i < max_words:\n        embedding_vector = embeddings_index.get(word)\n        if embedding_vector is not None:\n            embedding_matrix[i] = embedding_vector #Words not found in the embedding index will be all zeros. ",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "4676dd9eaa6ca2847d15b73768d4a5240535bf94",
        "trusted": true
      },
      "cell_type": "code",
      "source": "model = Sequential()\nmodel.add(Embedding(max_words, embedding_dim, input_length=maxlen))\nmodel.add(LSTM(dropout=0.5, units=1))\nmodel.summary()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "9040b2847c40ffbadbaaa65ab965f78e26833504",
        "trusted": true
      },
      "cell_type": "code",
      "source": "model.layers[0].set_weights([embedding_matrix])\nmodel.layers[0].trainable = False",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "a4ddd87505450787c6ac558b657dfeb916571d10",
        "trusted": true,
        "scrolled": true
      },
      "cell_type": "code",
      "source": "model.compile(optimizer='rmsprop',\nloss='binary_crossentropy',\nmetrics=['acc'])\n\nhistory = model.fit(x_train, y_train,\nepochs=10,\nbatch_size=32,\nvalidation_data=(x_val, y_val))\nmodel.save_weights('pre_trained_glove_model.h5')",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "17b4b10aa4c7a0e302c945174bd433db09fce1cc",
        "trusted": true
      },
      "cell_type": "code",
      "source": "acc = history.history['acc']\nval_acc = history.history['val_acc']\nloss = history.history['loss']\nval_loss = history.history['val_loss']\nepochs = range(1, len(acc) + 1)\nplt.plot(epochs, acc, 'bo', label='Training acc')\nplt.plot(epochs, val_acc, 'b', label='Validation acc')\nplt.title('Training and validation accuracy')\nplt.legend()\nplt.figure()\nplt.plot(epochs, loss, 'bo', label='Training loss')\nplt.plot(epochs, val_loss, 'b', label='Validation loss')\nplt.title('Training and validation loss')\nplt.legend()\nplt.show()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "bfb03e61735a6995c5804a83f5446a3c26baa4ef"
      },
      "cell_type": "code",
      "source": "",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.6.6",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 1
}