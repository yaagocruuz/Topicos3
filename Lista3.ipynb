{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import layers\n",
    "from keras import models\n",
    "from keras import losses\n",
    "from keras import optimizers\n",
    "\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.preprocessing import image\n",
    "\n",
    "from keras.datasets import mnist\n",
    "from keras.utils import to_categorical\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = models.Sequential()\n",
    "model.add(layers.Conv2D(filters= 32, kernel_size=(3, 3), activation='relu', input_shape=(28, 28, 1))) #(image_height, image_width, image_channels) (not including the batch dimension).\n",
    "model.add(layers.MaxPooling2D((2, 2)))\n",
    "model.add(layers.Conv2D(filters = 64, kernel_size=(3, 3), activation='relu'))\n",
    "model.add(layers.MaxPooling2D((2, 2)))\n",
    "model.add(layers.Conv2D(filters = 64, kernel_size=(3, 3), activation='relu'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_13 (Conv2D)           (None, 26, 26, 32)        320       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_9 (MaxPooling2 (None, 13, 13, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_14 (Conv2D)           (None, 11, 11, 64)        18496     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_10 (MaxPooling (None, 5, 5, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv2d_15 (Conv2D)           (None, 3, 3, 64)          36928     \n",
      "=================================================================\n",
      "Total params: 55,744\n",
      "Trainable params: 55,744\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.add(layers.Flatten()) # Output_shape=(None, 3*3*64)\n",
    "model.add(layers.Dense(units= 64, activation='relu'))\n",
    "model.add(layers.Dense(units=10, activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_13 (Conv2D)           (None, 26, 26, 32)        320       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_9 (MaxPooling2 (None, 13, 13, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_14 (Conv2D)           (None, 11, 11, 64)        18496     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_10 (MaxPooling (None, 5, 5, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv2d_15 (Conv2D)           (None, 3, 3, 64)          36928     \n",
      "_________________________________________________________________\n",
      "flatten_5 (Flatten)          (None, 576)               0         \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (None, 64)                36928     \n",
      "_________________________________________________________________\n",
      "dense_8 (Dense)              (None, 10)                650       \n",
      "=================================================================\n",
      "Total params: 93,322\n",
      "Trainable params: 93,322\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "(train_images, train_labels), (test_images, test_labels) = mnist.load_data()\n",
    "train_images = train_images.reshape((60000, 28, 28, 1))\n",
    "train_images = train_images.astype('float32') / 255\n",
    "test_images = test_images.reshape((10000, 28, 28, 1))\n",
    "test_images = test_images.astype('float32') / 255\n",
    "train_labels = to_categorical(train_labels) #Converts a class vector (integers) to binary class matrix.\n",
    "test_labels = to_categorical(test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = train_images[:6000, :, :]\n",
    "X_train_labels = train_labels[:6000, :]\n",
    "\n",
    "X_valid = train_images[6000:7000, :, :]\n",
    "X_valid_labels= train_labels[6000:7000, :]\n",
    "\n",
    "X_test = test_images[:1000, :, :]\n",
    "X_test_labels = test_labels[:1000, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(6000, 28, 28, 1)\n"
     ]
    }
   ],
   "source": [
    "print(X_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(6000, 10)\n"
     ]
    }
   ],
   "source": [
    "print(X_train_labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 6000 samples, validate on 1000 samples\n",
      "Epoch 1/10\n",
      "6000/6000 [==============================] - 4s 723us/step - loss: 0.8072 - acc: 0.7367 - val_loss: 0.3917 - val_acc: 0.8730\n",
      "Epoch 2/10\n",
      "6000/6000 [==============================] - 4s 626us/step - loss: 0.2458 - acc: 0.9253 - val_loss: 0.2173 - val_acc: 0.9280\n",
      "Epoch 3/10\n",
      "6000/6000 [==============================] - 4s 625us/step - loss: 0.1557 - acc: 0.9507 - val_loss: 0.1471 - val_acc: 0.9510\n",
      "Epoch 4/10\n",
      "6000/6000 [==============================] - 4s 637us/step - loss: 0.1213 - acc: 0.9613 - val_loss: 0.1413 - val_acc: 0.9590\n",
      "Epoch 5/10\n",
      "6000/6000 [==============================] - 4s 698us/step - loss: 0.0884 - acc: 0.9732 - val_loss: 0.1213 - val_acc: 0.9680\n",
      "Epoch 6/10\n",
      "6000/6000 [==============================] - 4s 706us/step - loss: 0.0707 - acc: 0.9785 - val_loss: 0.1048 - val_acc: 0.9680\n",
      "Epoch 7/10\n",
      "6000/6000 [==============================] - 4s 705us/step - loss: 0.0631 - acc: 0.9787 - val_loss: 0.1486 - val_acc: 0.9570\n",
      "Epoch 8/10\n",
      "6000/6000 [==============================] - 4s 699us/step - loss: 0.0552 - acc: 0.9840 - val_loss: 0.0944 - val_acc: 0.9720\n",
      "Epoch 9/10\n",
      "6000/6000 [==============================] - 4s 692us/step - loss: 0.0420 - acc: 0.9867 - val_loss: 0.0943 - val_acc: 0.9700\n",
      "Epoch 10/10\n",
      "6000/6000 [==============================] - 4s 691us/step - loss: 0.0343 - acc: 0.9858 - val_loss: 0.1403 - val_acc: 0.9600\n"
     ]
    }
   ],
   "source": [
    "model.compile(optimizer='rmsprop',\n",
    "loss='categorical_crossentropy',\n",
    "metrics=['accuracy'])\n",
    "history = model.fit(X_train, X_train_labels, epochs=10, batch_size=64, validation_data=(X_valid, X_valid_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000/1000 [==============================] - 0s 324us/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.963"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_loss, test_acc = model.evaluate(X_test, X_test_labels)\n",
    "test_acc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Primeiro modelo : logarithmic_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "model1 = models.Sequential()\n",
    "model1.add(layers.Conv2D(filters= 32, kernel_size=(3, 3), activation='relu', input_shape=(28, 28, 1))) #(image_height, image_width, image_channels) (not including the batch dimension).\n",
    "model1.add(layers.MaxPooling2D((2, 2)))\n",
    "model1.add(layers.Conv2D(filters = 64, kernel_size=(3, 3), activation='relu'))\n",
    "model1.add(layers.MaxPooling2D((2, 2)))\n",
    "model1.add(layers.Conv2D(filters = 64, kernel_size=(3, 3), activation='relu'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "model1.add(layers.Flatten()) # Output_shape=(None, 3*3*64)\n",
    "model1.add(layers.Dense(units= 64, activation='relu'))\n",
    "model1.add(layers.Dense(units=10, activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "(train_images, train_labels), (test_images, test_labels) = mnist.load_data()\n",
    "train_images = train_images.reshape((60000, 28, 28, 1))\n",
    "train_images = train_images.astype('float32') / 255\n",
    "test_images = test_images.reshape((10000, 28, 28, 1))\n",
    "test_images = test_images.astype('float32') / 255\n",
    "train_labels = to_categorical(train_labels) #Converts a class vector (integers) to binary class matrix.\n",
    "test_labels = to_categorical(test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = train_images[:6000, :, :]\n",
    "X_train_labels = train_labels[:6000, :]\n",
    "\n",
    "X_valid = train_images[6000:7000, :, :]\n",
    "X_valid_labels= train_labels[6000:7000, :]\n",
    "\n",
    "X_test = test_images[:1000, :, :]\n",
    "X_test_labels = test_labels[:1000, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 6000 samples, validate on 1000 samples\n",
      "Epoch 1/10\n",
      "6000/6000 [==============================] - 4s 745us/step - loss: 0.0233 - acc: 0.6593 - val_loss: 0.0055 - val_acc: 0.9290\n",
      "Epoch 2/10\n",
      "6000/6000 [==============================] - 4s 638us/step - loss: 0.0046 - acc: 0.9405 - val_loss: 0.0033 - val_acc: 0.9560\n",
      "Epoch 3/10\n",
      "6000/6000 [==============================] - 4s 665us/step - loss: 0.0031 - acc: 0.9590 - val_loss: 0.0026 - val_acc: 0.9680\n",
      "Epoch 4/10\n",
      "6000/6000 [==============================] - 4s 636us/step - loss: 0.0024 - acc: 0.9695 - val_loss: 0.0025 - val_acc: 0.9690\n",
      "Epoch 5/10\n",
      "6000/6000 [==============================] - 4s 649us/step - loss: 0.0021 - acc: 0.9738 - val_loss: 0.0028 - val_acc: 0.9640\n",
      "Epoch 6/10\n",
      "6000/6000 [==============================] - 4s 712us/step - loss: 0.0018 - acc: 0.9783 - val_loss: 0.0022 - val_acc: 0.9700\n",
      "Epoch 7/10\n",
      "6000/6000 [==============================] - 4s 711us/step - loss: 0.0014 - acc: 0.9823 - val_loss: 0.0020 - val_acc: 0.9770\n",
      "Epoch 8/10\n",
      "6000/6000 [==============================] - 4s 715us/step - loss: 0.0013 - acc: 0.9848 - val_loss: 0.0024 - val_acc: 0.9690\n",
      "Epoch 9/10\n",
      "6000/6000 [==============================] - 4s 709us/step - loss: 0.0012 - acc: 0.9858 - val_loss: 0.0021 - val_acc: 0.9730\n",
      "Epoch 10/10\n",
      "6000/6000 [==============================] - 4s 708us/step - loss: 0.0012 - acc: 0.9858 - val_loss: 0.0023 - val_acc: 0.9720\n"
     ]
    }
   ],
   "source": [
    "model1.compile(optimizer=optimizers.adagrad(),\n",
    "loss=losses.mean_squared_logarithmic_error,\n",
    "metrics=['accuracy'])\n",
    "history = model1.fit(X_train, X_train_labels, epochs=10, batch_size=64, validation_data=(X_valid, X_valid_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000/1000 [==============================] - 0s 318us/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.966"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_loss_le1, test_acc_le1 = model1.evaluate(X_test, X_test_labels)\n",
    "test_acc_le1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "model2 = models.Sequential()\n",
    "model2.add(layers.Conv2D(filters= 32, kernel_size=(3, 3), activation='relu', input_shape=(28, 28, 1))) #(image_height, image_width, image_channels) (not including the batch dimension).\n",
    "model2.add(layers.MaxPooling2D((2, 2)))\n",
    "model2.add(layers.Conv2D(filters = 64, kernel_size=(3, 3), activation='relu'))\n",
    "model2.add(layers.MaxPooling2D((2, 2)))\n",
    "model2.add(layers.Conv2D(filters = 64, kernel_size=(3, 3), activation='relu'))\n",
    "\n",
    "model2.add(layers.Flatten()) # Output_shape=(None, 3*3*64)\n",
    "model2.add(layers.Dense(units= 64, activation='relu'))\n",
    "model2.add(layers.Dense(units=10, activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 6000 samples, validate on 1000 samples\n",
      "Epoch 1/10\n",
      "6000/6000 [==============================] - 5s 835us/step - loss: 0.0025 - acc: 0.9648 - val_loss: 0.0034 - val_acc: 0.9570\n",
      "Epoch 2/10\n",
      "6000/6000 [==============================] - 4s 678us/step - loss: 0.0025 - acc: 0.9678 - val_loss: 0.0028 - val_acc: 0.9660\n",
      "Epoch 3/10\n",
      "6000/6000 [==============================] - 4s 646us/step - loss: 0.0021 - acc: 0.9742 - val_loss: 0.0023 - val_acc: 0.9710\n",
      "Epoch 4/10\n",
      "6000/6000 [==============================] - 4s 629us/step - loss: 0.0021 - acc: 0.9752 - val_loss: 0.0027 - val_acc: 0.9660\n",
      "Epoch 5/10\n",
      "6000/6000 [==============================] - 4s 667us/step - loss: 0.0023 - acc: 0.9730 - val_loss: 0.0035 - val_acc: 0.9590\n",
      "Epoch 6/10\n",
      "6000/6000 [==============================] - 4s 702us/step - loss: 0.0035 - acc: 0.9608 - val_loss: 0.0082 - val_acc: 0.9090\n",
      "Epoch 7/10\n",
      "6000/6000 [==============================] - 4s 732us/step - loss: 0.0058 - acc: 0.9368 - val_loss: 0.0056 - val_acc: 0.9410\n",
      "Epoch 8/10\n",
      "6000/6000 [==============================] - 4s 702us/step - loss: 0.0066 - acc: 0.9290 - val_loss: 0.0072 - val_acc: 0.9230\n",
      "Epoch 9/10\n",
      "6000/6000 [==============================] - 4s 708us/step - loss: 0.0070 - acc: 0.9257 - val_loss: 0.0070 - val_acc: 0.9260\n",
      "Epoch 10/10\n",
      "6000/6000 [==============================] - 4s 677us/step - loss: 0.0065 - acc: 0.9310 - val_loss: 0.0076 - val_acc: 0.9200\n"
     ]
    }
   ],
   "source": [
    "model2.compile(optimizer=optimizers.nadam(),\n",
    "loss=losses.mean_squared_logarithmic_error,\n",
    "metrics=['accuracy'])\n",
    "history = model2.fit(X_train, X_train_labels, epochs=10, batch_size=64, validation_data=(X_valid, X_valid_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000/1000 [==============================] - 0s 327us/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.865"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_loss_le2, test_acc_le2 = model2.evaluate(X_test, X_test_labels)\n",
    "test_acc_le2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "model3 = models.Sequential()\n",
    "model3.add(layers.Conv2D(filters= 32, kernel_size=(3, 3), activation='relu', input_shape=(28, 28, 1))) #(image_height, image_width, image_channels) (not including the batch dimension).\n",
    "model3.add(layers.MaxPooling2D((2, 2)))\n",
    "model3.add(layers.Conv2D(filters = 64, kernel_size=(3, 3), activation='relu'))\n",
    "model3.add(layers.MaxPooling2D((2, 2)))\n",
    "model3.add(layers.Conv2D(filters = 64, kernel_size=(3, 3), activation='relu'))\n",
    "\n",
    "model3.add(layers.Flatten()) # Output_shape=(None, 3*3*64)\n",
    "model3.add(layers.Dense(units= 64, activation='relu'))\n",
    "model3.add(layers.Dense(units=10, activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 6000 samples, validate on 1000 samples\n",
      "Epoch 1/10\n",
      "6000/6000 [==============================] - 5s 764us/step - loss: 0.0440 - acc: 0.0738 - val_loss: 0.0440 - val_acc: 0.0570\n",
      "Epoch 2/10\n",
      "6000/6000 [==============================] - 4s 673us/step - loss: 0.0440 - acc: 0.0765 - val_loss: 0.0440 - val_acc: 0.0590\n",
      "Epoch 3/10\n",
      "6000/6000 [==============================] - 4s 731us/step - loss: 0.0440 - acc: 0.0795 - val_loss: 0.0440 - val_acc: 0.0620\n",
      "Epoch 4/10\n",
      "6000/6000 [==============================] - 4s 681us/step - loss: 0.0440 - acc: 0.0813 - val_loss: 0.0440 - val_acc: 0.0650\n",
      "Epoch 5/10\n",
      "6000/6000 [==============================] - 4s 708us/step - loss: 0.0440 - acc: 0.0842 - val_loss: 0.0440 - val_acc: 0.0670\n",
      "Epoch 6/10\n",
      "6000/6000 [==============================] - 4s 708us/step - loss: 0.0440 - acc: 0.0858 - val_loss: 0.0440 - val_acc: 0.0710\n",
      "Epoch 7/10\n",
      "6000/6000 [==============================] - 4s 708us/step - loss: 0.0440 - acc: 0.0897 - val_loss: 0.0439 - val_acc: 0.0750\n",
      "Epoch 8/10\n",
      "6000/6000 [==============================] - 4s 698us/step - loss: 0.0439 - acc: 0.0915 - val_loss: 0.0439 - val_acc: 0.0770\n",
      "Epoch 9/10\n",
      "6000/6000 [==============================] - 5s 783us/step - loss: 0.0439 - acc: 0.0935 - val_loss: 0.0439 - val_acc: 0.0790\n",
      "Epoch 10/10\n",
      "6000/6000 [==============================] - 4s 709us/step - loss: 0.0439 - acc: 0.0972 - val_loss: 0.0439 - val_acc: 0.0880\n"
     ]
    }
   ],
   "source": [
    "model3.compile(optimizer=optimizers.SGD(),\n",
    "loss=losses.mean_squared_logarithmic_error,\n",
    "metrics=['accuracy'])\n",
    "history = model3.fit(X_train, X_train_labels, epochs=10, batch_size=64, validation_data=(X_valid, X_valid_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000/1000 [==============================] - 0s 341us/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.09"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_loss_le3, test_acc_le3 = model3.evaluate(X_test, X_test_labels)\n",
    "test_acc_le3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Segundo modelo: hinge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "model4 = models.Sequential()\n",
    "model4.add(layers.Conv2D(filters= 32, kernel_size=(3, 3), activation='relu', input_shape=(28, 28, 1))) #(image_height, image_width, image_channels) (not including the batch dimension).\n",
    "model4.add(layers.MaxPooling2D((2, 2)))\n",
    "model4.add(layers.Conv2D(filters = 64, kernel_size=(3, 3), activation='relu'))\n",
    "model4.add(layers.MaxPooling2D((2, 2)))\n",
    "model4.add(layers.Conv2D(filters = 64, kernel_size=(3, 3), activation='relu'))\n",
    "\n",
    "model4.add(layers.Flatten()) # Output_shape=(None, 3*3*64)\n",
    "model4.add(layers.Dense(units= 64, activation='relu'))\n",
    "model4.add(layers.Dense(units=10, activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 6000 samples, validate on 1000 samples\n",
      "Epoch 1/10\n",
      "6000/6000 [==============================] - 5s 780us/step - loss: 0.9648 - acc: 0.3573 - val_loss: 0.9351 - val_acc: 0.6540\n",
      "Epoch 2/10\n",
      "6000/6000 [==============================] - 4s 689us/step - loss: 0.9339 - acc: 0.6663 - val_loss: 0.9316 - val_acc: 0.6880\n",
      "Epoch 3/10\n",
      "6000/6000 [==============================] - 4s 722us/step - loss: 0.9323 - acc: 0.6800 - val_loss: 0.9309 - val_acc: 0.6930\n",
      "Epoch 4/10\n",
      "6000/6000 [==============================] - 4s 726us/step - loss: 0.9316 - acc: 0.6862 - val_loss: 0.9308 - val_acc: 0.6950\n",
      "Epoch 5/10\n",
      "6000/6000 [==============================] - 5s 839us/step - loss: 0.9313 - acc: 0.6887 - val_loss: 0.9302 - val_acc: 0.7010\n",
      "Epoch 6/10\n",
      "6000/6000 [==============================] - 5s 783us/step - loss: 0.9310 - acc: 0.6903 - val_loss: 0.9302 - val_acc: 0.7000\n",
      "Epoch 7/10\n",
      "6000/6000 [==============================] - 5s 773us/step - loss: 0.9309 - acc: 0.6923 - val_loss: 0.9301 - val_acc: 0.7030\n",
      "Epoch 8/10\n",
      "6000/6000 [==============================] - 5s 766us/step - loss: 0.9307 - acc: 0.6933 - val_loss: 0.9304 - val_acc: 0.6970\n",
      "Epoch 9/10\n",
      "6000/6000 [==============================] - 5s 784us/step - loss: 0.9306 - acc: 0.6950 - val_loss: 0.9303 - val_acc: 0.6980\n",
      "Epoch 10/10\n",
      "6000/6000 [==============================] - 5s 758us/step - loss: 0.9305 - acc: 0.6957 - val_loss: 0.9303 - val_acc: 0.6990\n"
     ]
    }
   ],
   "source": [
    "model4.compile(optimizer=optimizers.adagrad(),\n",
    "loss=losses.hinge,\n",
    "metrics=['accuracy'])\n",
    "history = model4.fit(X_train, X_train_labels, epochs=10, batch_size=64, validation_data=(X_valid, X_valid_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000/1000 [==============================] - 0s 331us/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.689"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_loss_le4, test_acc_le4 = model4.evaluate(X_test, X_test_labels)\n",
    "test_acc_le4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "model5 = models.Sequential()\n",
    "model5.add(layers.Conv2D(filters= 32, kernel_size=(3, 3), activation='relu', input_shape=(28, 28, 1))) #(image_height, image_width, image_channels) (not including the batch dimension).\n",
    "model5.add(layers.MaxPooling2D((2, 2)))\n",
    "model5.add(layers.Conv2D(filters = 64, kernel_size=(3, 3), activation='relu'))\n",
    "model5.add(layers.MaxPooling2D((2, 2)))\n",
    "model5.add(layers.Conv2D(filters = 64, kernel_size=(3, 3), activation='relu'))\n",
    "\n",
    "model5.add(layers.Flatten()) # Output_shape=(None, 3*3*64)\n",
    "model5.add(layers.Dense(units= 64, activation='relu'))\n",
    "model5.add(layers.Dense(units=10, activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 6000 samples, validate on 1000 samples\n",
      "Epoch 1/10\n",
      "6000/6000 [==============================] - 5s 852us/step - loss: 0.9626 - acc: 0.3845 - val_loss: 0.9465 - val_acc: 0.5360\n",
      "Epoch 2/10\n",
      "6000/6000 [==============================] - 4s 717us/step - loss: 0.9480 - acc: 0.5198 - val_loss: 0.9523 - val_acc: 0.4770\n",
      "Epoch 3/10\n",
      "6000/6000 [==============================] - 4s 681us/step - loss: 0.9504 - acc: 0.4960 - val_loss: 0.9537 - val_acc: 0.4630\n",
      "Epoch 4/10\n",
      "6000/6000 [==============================] - 4s 686us/step - loss: 0.9476 - acc: 0.5237 - val_loss: 0.9465 - val_acc: 0.5350\n",
      "Epoch 5/10\n",
      "6000/6000 [==============================] - 5s 766us/step - loss: 0.9557 - acc: 0.4430 - val_loss: 0.9541 - val_acc: 0.4590\n",
      "Epoch 6/10\n",
      "6000/6000 [==============================] - 5s 823us/step - loss: 0.9505 - acc: 0.4948 - val_loss: 0.9459 - val_acc: 0.5410\n",
      "Epoch 7/10\n",
      "6000/6000 [==============================] - 5s 769us/step - loss: 0.9570 - acc: 0.4303 - val_loss: 0.9591 - val_acc: 0.4090\n",
      "Epoch 8/10\n",
      "6000/6000 [==============================] - 5s 788us/step - loss: 0.9606 - acc: 0.3938 - val_loss: 0.9779 - val_acc: 0.2210\n",
      "Epoch 9/10\n",
      "6000/6000 [==============================] - 5s 810us/step - loss: 0.9735 - acc: 0.2647 - val_loss: 0.9642 - val_acc: 0.3580\n",
      "Epoch 10/10\n",
      "6000/6000 [==============================] - 5s 828us/step - loss: 0.9592 - acc: 0.4082 - val_loss: 0.9496 - val_acc: 0.5040\n"
     ]
    }
   ],
   "source": [
    "model5.compile(optimizer=optimizers.nadam(),\n",
    "loss=losses.hinge,\n",
    "metrics=['accuracy'])\n",
    "history = model5.fit(X_train, X_train_labels, epochs=10, batch_size=64, validation_data=(X_valid, X_valid_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000/1000 [==============================] - 0s 453us/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.524"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_loss_le5, test_acc_le5 = model5.evaluate(X_test, X_test_labels)\n",
    "test_acc_le5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "model6 = models.Sequential()\n",
    "model6.add(layers.Conv2D(filters= 32, kernel_size=(3, 3), activation='relu', input_shape=(28, 28, 1))) #(image_height, image_width, image_channels) (not including the batch dimension).\n",
    "model6.add(layers.MaxPooling2D((2, 2)))\n",
    "model6.add(layers.Conv2D(filters = 64, kernel_size=(3, 3), activation='relu'))\n",
    "model6.add(layers.MaxPooling2D((2, 2)))\n",
    "model6.add(layers.Conv2D(filters = 64, kernel_size=(3, 3), activation='relu'))\n",
    "\n",
    "model6.add(layers.Flatten()) # Output_shape=(None, 3*3*64)\n",
    "model6.add(layers.Dense(units= 64, activation='relu'))\n",
    "model6.add(layers.Dense(units=10, activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 6000 samples, validate on 1000 samples\n",
      "Epoch 1/10\n",
      "6000/6000 [==============================] - 6s 966us/step - loss: 0.9900 - acc: 0.0972 - val_loss: 0.9900 - val_acc: 0.0990\n",
      "Epoch 2/10\n",
      "6000/6000 [==============================] - 5s 764us/step - loss: 0.9900 - acc: 0.0977 - val_loss: 0.9900 - val_acc: 0.0990\n",
      "Epoch 3/10\n",
      "6000/6000 [==============================] - 5s 764us/step - loss: 0.9900 - acc: 0.0990 - val_loss: 0.9900 - val_acc: 0.0990\n",
      "Epoch 4/10\n",
      "6000/6000 [==============================] - 5s 805us/step - loss: 0.9900 - acc: 0.0997 - val_loss: 0.9900 - val_acc: 0.0990\n",
      "Epoch 5/10\n",
      "6000/6000 [==============================] - 5s 875us/step - loss: 0.9900 - acc: 0.0997 - val_loss: 0.9899 - val_acc: 0.0990\n",
      "Epoch 6/10\n",
      "6000/6000 [==============================] - 6s 981us/step - loss: 0.9900 - acc: 0.1000 - val_loss: 0.9899 - val_acc: 0.0990\n",
      "Epoch 7/10\n",
      "6000/6000 [==============================] - 5s 778us/step - loss: 0.9899 - acc: 0.1005 - val_loss: 0.9899 - val_acc: 0.0990\n",
      "Epoch 8/10\n",
      "6000/6000 [==============================] - 5s 796us/step - loss: 0.9899 - acc: 0.1008 - val_loss: 0.9899 - val_acc: 0.1000\n",
      "Epoch 9/10\n",
      "6000/6000 [==============================] - 5s 769us/step - loss: 0.9899 - acc: 0.1012 - val_loss: 0.9899 - val_acc: 0.1000\n",
      "Epoch 10/10\n",
      "6000/6000 [==============================] - 5s 760us/step - loss: 0.9899 - acc: 0.1015 - val_loss: 0.9899 - val_acc: 0.1000\n"
     ]
    }
   ],
   "source": [
    "model6.compile(optimizer=optimizers.SGD(),\n",
    "loss=losses.hinge,\n",
    "metrics=['accuracy'])\n",
    "history = model6.fit(X_train, X_train_labels, epochs=10, batch_size=64, validation_data=(X_valid, X_valid_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000/1000 [==============================] - 0s 406us/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.091"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_loss_le6, test_acc_le6 = model6.evaluate(X_test, X_test_labels)\n",
    "test_acc_le6"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Terceiro modelo: poisson"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "model7 = models.Sequential()\n",
    "model7.add(layers.Conv2D(filters= 32, kernel_size=(3, 3), activation='relu', input_shape=(28, 28, 1))) #(image_height, image_width, image_channels) (not including the batch dimension).\n",
    "model7.add(layers.MaxPooling2D((2, 2)))\n",
    "model7.add(layers.Conv2D(filters = 64, kernel_size=(3, 3), activation='relu'))\n",
    "model7.add(layers.MaxPooling2D((2, 2)))\n",
    "model7.add(layers.Conv2D(filters = 64, kernel_size=(3, 3), activation='relu'))\n",
    "\n",
    "model7.add(layers.Flatten()) # Output_shape=(None, 3*3*64)\n",
    "model7.add(layers.Dense(units= 64, activation='relu'))\n",
    "model7.add(layers.Dense(units=10, activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 6000 samples, validate on 1000 samples\n",
      "Epoch 1/10\n",
      "6000/6000 [==============================] - 5s 916us/step - loss: 0.1709 - acc: 0.7878 - val_loss: 0.1188 - val_acc: 0.9420\n",
      "Epoch 2/10\n",
      "6000/6000 [==============================] - 4s 747us/step - loss: 0.1164 - acc: 0.9528 - val_loss: 0.1116 - val_acc: 0.9680\n",
      "Epoch 3/10\n",
      "6000/6000 [==============================] - 4s 740us/step - loss: 0.1108 - acc: 0.9658 - val_loss: 0.1126 - val_acc: 0.9630\n",
      "Epoch 4/10\n",
      "6000/6000 [==============================] - 5s 792us/step - loss: 0.1087 - acc: 0.9738 - val_loss: 0.1100 - val_acc: 0.9700\n",
      "Epoch 5/10\n",
      "6000/6000 [==============================] - 5s 818us/step - loss: 0.1070 - acc: 0.9787 - val_loss: 0.1086 - val_acc: 0.9740\n",
      "Epoch 6/10\n",
      "6000/6000 [==============================] - 5s 764us/step - loss: 0.1057 - acc: 0.9805 - val_loss: 0.1091 - val_acc: 0.9740\n",
      "Epoch 7/10\n",
      "6000/6000 [==============================] - 5s 750us/step - loss: 0.1046 - acc: 0.9852 - val_loss: 0.1094 - val_acc: 0.9750\n",
      "Epoch 8/10\n",
      "6000/6000 [==============================] - 4s 692us/step - loss: 0.1041 - acc: 0.9875 - val_loss: 0.1077 - val_acc: 0.9790\n",
      "Epoch 9/10\n",
      "6000/6000 [==============================] - 4s 692us/step - loss: 0.1033 - acc: 0.9910 - val_loss: 0.1079 - val_acc: 0.9760\n",
      "Epoch 10/10\n",
      "6000/6000 [==============================] - 4s 707us/step - loss: 0.1029 - acc: 0.9913 - val_loss: 0.1097 - val_acc: 0.9740\n"
     ]
    }
   ],
   "source": [
    "model7.compile(optimizer=optimizers.adagrad(),\n",
    "loss=losses.poisson,\n",
    "metrics=['accuracy'])\n",
    "history = model7.fit(X_train, X_train_labels, epochs=10, batch_size=64, validation_data=(X_valid, X_valid_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000/1000 [==============================] - 0s 312us/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.974"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_loss_le7, test_acc_le7 = model7.evaluate(X_test, X_test_labels)\n",
    "test_acc_le7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "model8 = models.Sequential()\n",
    "model8.add(layers.Conv2D(filters= 32, kernel_size=(3, 3), activation='relu', input_shape=(28, 28, 1))) #(image_height, image_width, image_channels) (not including the batch dimension).\n",
    "model8.add(layers.MaxPooling2D((2, 2)))\n",
    "model8.add(layers.Conv2D(filters = 64, kernel_size=(3, 3), activation='relu'))\n",
    "model8.add(layers.MaxPooling2D((2, 2)))\n",
    "model8.add(layers.Conv2D(filters = 64, kernel_size=(3, 3), activation='relu'))\n",
    "\n",
    "model8.add(layers.Flatten()) # Output_shape=(None, 3*3*64)\n",
    "model8.add(layers.Dense(units= 64, activation='relu'))\n",
    "model8.add(layers.Dense(units=10, activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 6000 samples, validate on 1000 samples\n",
      "Epoch 1/10\n",
      "6000/6000 [==============================] - 5s 871us/step - loss: 0.1650 - acc: 0.7958 - val_loss: 0.1288 - val_acc: 0.9130\n",
      "Epoch 2/10\n",
      "6000/6000 [==============================] - 4s 698us/step - loss: 0.1152 - acc: 0.9555 - val_loss: 0.1143 - val_acc: 0.9560\n",
      "Epoch 3/10\n",
      "6000/6000 [==============================] - 4s 683us/step - loss: 0.1095 - acc: 0.9722 - val_loss: 0.1128 - val_acc: 0.9650\n",
      "Epoch 4/10\n",
      "6000/6000 [==============================] - 4s 698us/step - loss: 0.1067 - acc: 0.9783 - val_loss: 0.1096 - val_acc: 0.9780\n",
      "Epoch 5/10\n",
      "6000/6000 [==============================] - 5s 792us/step - loss: 0.1046 - acc: 0.9868 - val_loss: 0.1169 - val_acc: 0.9600\n",
      "Epoch 6/10\n",
      "6000/6000 [==============================] - 5s 836us/step - loss: 0.1035 - acc: 0.9888 - val_loss: 0.1095 - val_acc: 0.9750\n",
      "Epoch 7/10\n",
      "6000/6000 [==============================] - 5s 764us/step - loss: 0.1025 - acc: 0.9928 - val_loss: 0.1125 - val_acc: 0.9730\n",
      "Epoch 8/10\n",
      "6000/6000 [==============================] - 5s 835us/step - loss: 0.1020 - acc: 0.9940 - val_loss: 0.1117 - val_acc: 0.9750\n",
      "Epoch 9/10\n",
      "6000/6000 [==============================] - 5s 829us/step - loss: 0.1017 - acc: 0.9940 - val_loss: 0.1084 - val_acc: 0.9810\n",
      "Epoch 10/10\n",
      "6000/6000 [==============================] - 5s 800us/step - loss: 0.1062 - acc: 0.9835 - val_loss: 0.1084 - val_acc: 0.9780\n"
     ]
    }
   ],
   "source": [
    "model8.compile(optimizer=optimizers.nadam(),\n",
    "loss=losses.poisson,\n",
    "metrics=['accuracy'])\n",
    "history = model8.fit(X_train, X_train_labels, epochs=10, batch_size=64, validation_data=(X_valid, X_valid_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000/1000 [==============================] - 0s 315us/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.974"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_loss_le8, test_acc_le8 = model8.evaluate(X_test, X_test_labels)\n",
    "test_acc_le8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "model9 = models.Sequential()\n",
    "model9.add(layers.Conv2D(filters= 32, kernel_size=(3, 3), activation='relu', input_shape=(28, 28, 1))) #(image_height, image_width, image_channels) (not including the batch dimension).\n",
    "model9.add(layers.MaxPooling2D((2, 2)))\n",
    "model9.add(layers.Conv2D(filters = 64, kernel_size=(3, 3), activation='relu'))\n",
    "model9.add(layers.MaxPooling2D((2, 2)))\n",
    "model9.add(layers.Conv2D(filters = 64, kernel_size=(3, 3), activation='relu'))\n",
    "\n",
    "model9.add(layers.Flatten()) # Output_shape=(None, 3*3*64)\n",
    "model9.add(layers.Dense(units= 64, activation='relu'))\n",
    "model9.add(layers.Dense(units=10, activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 6000 samples, validate on 1000 samples\n",
      "Epoch 1/10\n",
      "6000/6000 [==============================] - 6s 932us/step - loss: 0.3306 - acc: 0.1363 - val_loss: 0.3303 - val_acc: 0.1500\n",
      "Epoch 2/10\n",
      "6000/6000 [==============================] - 5s 767us/step - loss: 0.3300 - acc: 0.1658 - val_loss: 0.3297 - val_acc: 0.1750\n",
      "Epoch 3/10\n",
      "6000/6000 [==============================] - 5s 813us/step - loss: 0.3295 - acc: 0.1943 - val_loss: 0.3292 - val_acc: 0.2040\n",
      "Epoch 4/10\n",
      "6000/6000 [==============================] - 5s 811us/step - loss: 0.3290 - acc: 0.2077 - val_loss: 0.3287 - val_acc: 0.2220\n",
      "Epoch 5/10\n",
      "6000/6000 [==============================] - 5s 760us/step - loss: 0.3285 - acc: 0.2308 - val_loss: 0.3282 - val_acc: 0.2400\n",
      "Epoch 6/10\n",
      "6000/6000 [==============================] - 5s 763us/step - loss: 0.3280 - acc: 0.2603 - val_loss: 0.3277 - val_acc: 0.2600\n",
      "Epoch 7/10\n",
      "6000/6000 [==============================] - 5s 767us/step - loss: 0.3274 - acc: 0.2885 - val_loss: 0.3271 - val_acc: 0.2800\n",
      "Epoch 8/10\n",
      "6000/6000 [==============================] - 5s 772us/step - loss: 0.3268 - acc: 0.3083 - val_loss: 0.3265 - val_acc: 0.2910\n",
      "Epoch 9/10\n",
      "6000/6000 [==============================] - 4s 749us/step - loss: 0.3262 - acc: 0.3235 - val_loss: 0.3259 - val_acc: 0.2980\n",
      "Epoch 10/10\n",
      "6000/6000 [==============================] - 5s 752us/step - loss: 0.3255 - acc: 0.3313 - val_loss: 0.3252 - val_acc: 0.3040\n"
     ]
    }
   ],
   "source": [
    "model9.compile(optimizer=optimizers.SGD(),\n",
    "loss=losses.poisson,\n",
    "metrics=['accuracy'])\n",
    "history = model9.fit(X_train, X_train_labels, epochs=10, batch_size=64, validation_data=(X_valid, X_valid_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000/1000 [==============================] - 0s 313us/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.284"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_loss_le9, test_acc_le9 = model9.evaluate(X_test, X_test_labels)\n",
    "test_acc_le9"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modelo 1 foi o melhor. Loss = mean_squared_logarithmic_error, optimizer = adagrad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "datagen = ImageDataGenerator(\n",
    "rotation_range=40,\n",
    "width_shift_range=0.2,\n",
    "height_shift_range=0.2,\n",
    "shear_range=0.2,\n",
    "zoom_range=0.2,\n",
    "horizontal_flip=True,\n",
    "fill_mode='nearest')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "imgplot = plt.imshow(image.array_to_img(X_train[0]))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAP8AAAD8CAYAAAC4nHJkAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDIuMi4yLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvhp/UCwAAD+JJREFUeJzt3X+MVeWdx/HPl1FAQVRAWIKIWlGrGJEMaKhZ0VXjagM0pAZMlOoq9Qdmm+wfq8akJqamGtvdjX800pRItbWtwR+TarQNMWJ0RQZCihWxQFg7goD8UATBOHz3jzk0o855zuX+Onfm+34lZmbu5545jxc+nHvvc895zN0FIJ5BZQ8AQDkoPxAU5QeCovxAUJQfCIryA0FRfiAoyg8ERfmBoI5p5s7MjI8TAg3m7lbJ/Wo68pvZNWa2wcw2mtk9tfwuAM1l1X6238zaJL0v6SpJXZJWSZrv7u8mtuHIDzRYM4780yVtdPfN7v6FpN9Jml3D7wPQRLWUf7ykv/f6uSu77SvMbKGZdZpZZw37AlBntbzh19dTi288rXf3xZIWSzztB1pJLUf+LkkTev18qqSttQ0HQLPUUv5VkiaZ2RlmNljSPEkd9RkWgEar+mm/u39pZoskvSKpTdISd/9r3UaGltDW1pbMR48ency7u7tzs48//riqMaE+qp7qq2pnvObvdyh//9OUD/kA6L8oPxAU5QeCovxAUJQfCIryA0E19Xx+9D8TJ05M5jfffHMynzdvXm62a9eu5LbPP/98Mn/qqaeSeVdXVzKPjiM/EBTlB4Ki/EBQlB8IivIDQVF+ICim+oIbPnx4Mr/77ruT+Y033pjMTzrppNzsrLPOSm47ffr0ZF40DfnEE0/kZps2bUpuG+GMQ478QFCUHwiK8gNBUX4gKMoPBEX5gaAoPxAU8/wD3LHHHpvMi+b5R40alcxT8/iSZJZ/IdmiK0cfPnw4md92223JfMaMGbnZY489ltz2ySefTOaHDh1K5v0BR34gKMoPBEX5gaAoPxAU5QeCovxAUJQfCKqmeX4z2yJpn6RuSV+6e3s9BoWjc+qpp+Zm06ZNS247f/78ZH7ppZdWNaYjDh48mJsdOHAgue3gwYOT+bBhw5L52WefnZtde+21yW1ff/31ZL5hw4Zk3h/U40M+l7v7wL/yATDA8LQfCKrW8rukP5nZajNbWI8BAWiOWp/2f8fdt5rZGEl/NrP33H1F7ztk/yjwDwPQYmo68rv71uzrDknPSfrGFRfdfbG7t/NmINBaqi6/mQ0zsxOOfC/paknv1GtgABqrlqf9YyU9l52yeYyk37r7y3UZFYCGq7r87r5Z0oV1HEtYgwaln4C1tbUl8zlz5uRmixYtSm57xhln1LTvIo8//nhutnz58uS29957bzIv+gxD6loGqXP9JenCC9N/tTdu3JjMu7u7k3krYKoPCIryA0FRfiAoyg8ERfmBoCg/EJQVXT65rjsza97OWkjRqakXXHBBMr/hhhuS+axZs3Kzoqm8oj//PXv2JPOiU1/nzp2bmxWdkvvQQw8l8zvvvDOZpy4bXjQV9+abbybzyy+/PJmXyd3z/8d74cgPBEX5gaAoPxAU5QeCovxAUJQfCIryA0GxRHcTXHnllcn8/vvvT+ZFp5cOGTIkN9u7d29y21dffTWZd3R0JPMVK1Yk85Siufa1a9cm83379iXzESNGVL3vt99+O5kPBBz5gaAoPxAU5QeCovxAUJQfCIryA0FRfiAo5vkrdMwx+Q/VggULktvecsstyXzq1KlV77vIe++9l8wfeeSRZL5u3bpknlqCu0jRtitXrkzmmzdvTuZTpkzJzYouST58+PBkfsoppyTznTt3JvNWwJEfCIryA0FRfiAoyg8ERfmBoCg/EBTlB4IqnEA2syWSvitph7tPzm4bKen3kk6XtEXS9e6evsB7P5da0vmOO+5IbnvRRRfVtO8tW7Yk8/379+dmRctgr1q1qpoh1UXquvqSdPjw4WQ+efLkhu17woQJyfyTTz6pet+topIj/xOSrvnabfdIWu7ukyQtz34G0I8Ult/dV0ja/bWbZ0tamn2/VNKcOo8LQINV+5p/rLtvk6Ts65j6DQlAMzT8s/1mtlDSwkbvB8DRqfbIv93MxklS9nVH3h3dfbG7t7t7e5X7AtAA1Za/Q9KRU9kWSHqhPsMB0CyF5TezpyX9r6RzzKzLzP5N0k8lXWVmf5N0VfYzgH6k8DW/u8/Pif6lzmNpqKLzsy+++OJkfuutt+Zm5513XnJbd0/ma9asSeaPPvpoMv/ggw9ys7feeiu5bZmK5tpHjhyZzDs7O5P5JZdckpsNGpQ+7o0dOzaZT5s2LZm/8cYbybwV8Ak/ICjKDwRF+YGgKD8QFOUHgqL8QFBhLt1dNHVz1113JfPrrrsuNyu6DPTWrVuTedEy16tXr07mmzZtSuatquiU3aFDhybzwYMH13M4X/Huu+8m8+3btzds383CkR8IivIDQVF+ICjKDwRF+YGgKD8QFOUHggozzz9q1KhkPnPmzGReNJef8tprryXzBx98MJl/+umnVe+7PzvttNOSedEl0VOn7RadZl00z79x48Zk3h9w5AeCovxAUJQfCIryA0FRfiAoyg8ERfmBoAbMPP8JJ5yQzKdOnZrMR4wYUfW+P/roo2S+du3aZN6fl3suuvx26pz79vb0Ik5XX311Tfv+/PPPc7OiP5MNGzYk84GAIz8QFOUHgqL8QFCUHwiK8gNBUX4gKMoPBFU4z29mSyR9V9IOd5+c3faApNsk7czudp+7v9SoQVbiuOOOS+bnnHNOMv/iiy+SeWq+uuhc/6KlpseMGZPMd+zYkcxTaxIUXRv/wIEDyXzcuHHJvOjzEXPnzs3NrrjiiuS25557bjI/ePBgMn/mmWdysxdffDG5bSsvbV4vlRz5n5B0TR+3/5e7T8n+K7X4AI5eYfndfYWk3U0YC4AmquU1/yIz+4uZLTGzk+s2IgBNUW35fyHpW5KmSNom6Wd5dzSzhWbWaWadVe4LQANUVX533+7u3e5+WNIvJU1P3Hexu7e7e/osDgBNVVX5zaz3W8Dfk/ROfYYDoFkqmep7WtJMSaPNrEvSjyXNNLMpklzSFkk/bOAYATSAFV2/vK47M2vYzorO7b7pppuS+e23357Mp02blpt1d3cnt92zZ08yL7ou/5o1a5L5rl27crMTTzwxue3+/fuT+ejRo5P5mWeemcwnTZqUmw0ZMiS5bdGf6e7d6UmoGTNm5GabN29Oblv0+YhW5u7pBy7DJ/yAoCg/EBTlB4Ki/EBQlB8IivIDQQ2YS3cXTVm+/PLLybxoWun888/PzY4//vjktkXTZUX5+PHjk3nK0KFDk3nq8taVbF+Lzz77LJm///77ybyjoyOZD4RltBuJIz8QFOUHgqL8QFCUHwiK8gNBUX4gKMoPBDVgTumtVdFc+sMPP5ybzZs3r97DqZuizy8UKVo+vOh05q6urtzspZfSF31+5ZVXknlnZ/rKcEWXJR+oOKUXQBLlB4Ki/EBQlB8IivIDQVF+ICjKDwTFPH+FBg3K/3dy9uzZyW2L8lmzZiXzomWwU/PdEydOTG67c+fOZF60lHXRMtnLli3LzT788MPktnv37k3mzfy7258wzw8gifIDQVF+ICjKDwRF+YGgKD8QFOUHgiqc5zezCZJ+LemfJB2WtNjd/8fMRkr6vaTTJW2RdL27J9ei7s/z/GW67LLLkvmhQ4dys6J5+KLr8q9fvz6ZF53vj+ar5zz/l5L+w92/LekSSXeZ2XmS7pG03N0nSVqe/Qygnygsv7tvc/c12ff7JK2XNF7SbElLs7stlTSnUYMEUH9H9ZrfzE6XdJGklZLGuvs2qecfCElj6j04AI1T8Vp9ZjZc0jJJP3L3Tyu9NpyZLZS0sLrhAWiUio78Znaseor/G3d/Nrt5u5mNy/Jxknb0ta27L3b3dndvr8eAAdRHYfmt5xD/K0nr3f3nvaIOSQuy7xdIeqH+wwPQKJVM9V0q6XVJ69Qz1SdJ96nndf8fJJ0m6QNJ33f33QW/i6m+FtPW1pbMiy7NjdZT6VQf5/MHR/kHHs7nB5BE+YGgKD8QFOUHgqL8QFCUHwiq4o/3YmBiKi8ujvxAUJQfCIryA0FRfiAoyg8ERfmBoCg/EBTlB4Ki/EBQlB8IivIDQVF+ICjKDwRF+YGgKD8QFOUHgqL8QFCUHwiK8gNBUX4gKMoPBEX5gaAoPxBUYfnNbIKZvWpm683sr2b279ntD5jZh2a2Nvvv2sYPF0C9mLun72A2TtI4d19jZidIWi1pjqTrJX3m7o9WvDOz9M4A1MzdrZL7Fa7Y4+7bJG3Lvt9nZuslja9teADKdlSv+c3sdEkXSVqZ3bTIzP5iZkvM7OScbRaaWaeZddY0UgB1Vfi0/x93NBsu6TVJP3H3Z81srKSPJbmkB9Xz0uCWgt/B036gwSp92l9R+c3sWEl/lPSKu/+8j/x0SX9098kFv4fyAw1WafkrebffJP1K0vrexc/eCDzie5LeOdpBAihPJe/2XyrpdUnrJB3Obr5P0nxJU9TztH+LpB9mbw6mfhdHfqDB6vq0v14oP9B4dXvaD2BgovxAUJQfCIryA0FRfiAoyg8ERfmBoCg/EBTlB4Ki/EBQlB8IivIDQVF+ICjKDwRVeAHPOvtY0v/1+nl0dlsratWxteq4JMZWrXqObWKld2zq+fzf2LlZp7u3lzaAhFYdW6uOS2Js1SprbDztB4Ki/EBQZZd/ccn7T2nVsbXquCTGVq1Sxlbqa34A5Sn7yA+gJKWU38yuMbMNZrbRzO4pYwx5zGyLma3LVh4udYmxbBm0HWb2Tq/bRprZn83sb9nXPpdJK2lsLbFyc2Jl6VIfu1Zb8brpT/vNrE3S+5KuktQlaZWk+e7+blMHksPMtkhqd/fS54TN7J8lfSbp10dWQzKzRyTtdvefZv9wnuzu/9kiY3tAR7lyc4PGlrey9A9U4mNXzxWv66GMI/90SRvdfbO7fyHpd5JmlzCOlufuKyTt/trNsyUtzb5fqp6/PE2XM7aW4O7b3H1N9v0+SUdWli71sUuMqxRllH+8pL/3+rlLrbXkt0v6k5mtNrOFZQ+mD2OPrIyUfR1T8ni+rnDl5mb62srSLfPYVbPidb2VUf6+VhNppSmH77j7VEn/Kumu7OktKvMLSd9SzzJu2yT9rMzBZCtLL5P0I3f/tMyx9NbHuEp53Moof5ekCb1+PlXS1hLG0Sd335p93SHpOfW8TGkl248skpp93VHyeP7B3be7e7e7H5b0S5X42GUrSy+T9Bt3fza7ufTHrq9xlfW4lVH+VZImmdkZZjZY0jxJHSWM4xvMbFj2RozMbJikq9V6qw93SFqQfb9A0gsljuUrWmXl5ryVpVXyY9dqK16X8iGfbCrjvyW1SVri7j9p+iD6YGZnqudoL/Wc8fjbMsdmZk9Lmqmes762S/qxpOcl/UHSaZI+kPR9d2/6G285Y5upo1y5uUFjy1tZeqVKfOzqueJ1XcbDJ/yAmPiEHxAU5QeCovxAUJQfCIryA0FRfiAoyg8ERfmBoP4fKRnBpp9HlToAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "i = 0;\n",
    "for batch in datagen.flow(X_train, batch_size=20):\n",
    "    plt.figure(i)\n",
    "    imgplot = plt.imshow(image.array_to_img(batch[0]))\n",
    "    i += 1\n",
    "    if i % 4 == 0:\n",
    "        break\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "modelf = models.Sequential()\n",
    "modelf.add(layers.Conv2D(filters= 32, kernel_size=(3, 3), activation='relu', input_shape=(28, 28, 1))) #(image_height, image_width, image_channels) (not including the batch dimension).\n",
    "modelf.add(layers.MaxPooling2D((2, 2)))\n",
    "modelf.add(layers.Conv2D(filters = 64, kernel_size=(3, 3), activation='relu'))\n",
    "modelf.add(layers.MaxPooling2D((2, 2)))\n",
    "modelf.add(layers.Conv2D(filters = 64, kernel_size=(3, 3), activation='relu'))\n",
    "modelf.add(layers.Flatten()) # Output_shape=(None, 3*3*64)\n",
    "modelf.add(layers.Dropout(0.5))\n",
    "modelf.add(layers.Dense(units= 64, activation='relu'))\n",
    "modelf.add(layers.Dense(units=10, activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "modelf.compile(optimizer=optimizers.adagrad(),\n",
    "loss=losses.mean_squared_logarithmic_error,\n",
    "metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "500/500 [==============================] - 10s 20ms/step - loss: 1.2736 - acc: 0.5695 - val_loss: 0.3775 - val_acc: 0.9030\n",
      "Epoch 2/5\n",
      "500/500 [==============================] - 10s 20ms/step - loss: 0.9943 - acc: 0.6742 - val_loss: 0.2821 - val_acc: 0.9160\n",
      "Epoch 3/5\n",
      "500/500 [==============================] - 11s 22ms/step - loss: 0.8353 - acc: 0.7338 - val_loss: 0.2389 - val_acc: 0.9300\n",
      "Epoch 4/5\n",
      "500/500 [==============================] - 10s 20ms/step - loss: 0.7255 - acc: 0.7642 - val_loss: 0.2141 - val_acc: 0.9350\n",
      "Epoch 5/5\n",
      "500/500 [==============================] - 10s 20ms/step - loss: 0.6622 - acc: 0.7893 - val_loss: 0.2076 - val_acc: 0.9390\n"
     ]
    }
   ],
   "source": [
    "train_datagen = ImageDataGenerator(\n",
    "rotation_range=40,\n",
    "width_shift_range=0.2,\n",
    "height_shift_range=0.2,\n",
    "shear_range=0.2,\n",
    "zoom_range=0.2,\n",
    "horizontal_flip=True,)\n",
    "\n",
    "history = model.fit_generator(datagen.flow(X_train, X_train_labels, batch_size=12), epochs=5, validation_data=(X_valid, X_valid_labels))\n",
    "#Generate batches of tensor image data with real-time data augmentation. (6000/12 = 500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000/1000 [==============================] - 1s 529us/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.971"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_lossf, test_accf = modelf.evaluate(X_test, X_test_labels)\n",
    "test_accf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
